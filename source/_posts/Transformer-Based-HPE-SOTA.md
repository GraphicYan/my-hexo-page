---
title: 基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望
layout: Transformer Based Human Pose Estimation -- Some SOTAs
toc: true
top: true
tag: AI
date: 2024/10/05 15:56:53
---

## 原创性声明
本文为作者原创，在个人Blog首次发布，如需转载请注明引用出处。（yanzhang.cg@gmail.com 或 https://graphicyan.github.io/）


## 一、引言

前两篇文章中，我准备在3D人形动作领域结合数据和网络的优势来落地一些技术尝试。实际上，随着深度学习技术的发展，尤其是Transformer架构的引入，人体姿态估计领域已经取得了一些显著进展。本文就详细介绍三个基于Transformer的最新SOTA（State-of-the-Art）工作：**HMR2.0**、**Multi-HMR**和**GVHMR**，并按照发表时间顺序进行讨论。每个方法的技术原理、优势、训练数据集及配置都将被详细讲解。


---

## 二、SOTA方法详解

### 1. HMR2.0 (2023-05-31)

#### 核心原理
- **ViT Backbone**: 引入了Vision Transformer (ViT) 作为特征提取器，替代了传统的卷积神经网络(CNN)，以捕捉更丰富的全局信息。
- **Temporal Cues**: 利用时间线索直接从视频中预测骨架姿态序列，通过深度网络预测骨骼姿态序列。
- **End-to-end Learning**: 整个过程从图像输入到SMPL参数输出完全端到端训练，简化了流程，并提升了模型的鲁棒性。

#### 技术优势
- **精度提升**: 在Human3.6M数据集上，MPJPE从原始HMR的58.7mm降至42.3mm。
- **鲁棒性增强**: 通过Transformer的全局建模能力，显著改善遮挡场景下的预测效果。
- **轻量化设计**: 尽管精度有所提高，但推理速度并未显著下降。

#### 缺点分析
- **对复杂遮挡处理能力有限**：尽管HMR2.0通过Transformer架构显著提升了对遮挡场景的鲁棒性，但在极端遮挡情况下（如多人重叠或物体遮挡），其表现仍不如预期。
- **依赖高质量标注数据**：HMR2.0的训练高度依赖于大规模高质量的动作捕捉数据集（如Human3.6M和MPI-INF-3DHP），对于缺乏此类数据的新应用场景，模型泛化能力可能受限。
- **实时性能有待提升**：虽然HMR2.0在精度上有了显著提升，但为了达到更高的准确度，模型复杂度增加，导致推理速度相对较慢，难以满足某些实时应用的需求。

#### 数据集
- **训练数据集**:
  - **Human3.6M [Ionescu et al. 2014]**：包含专业演员在实验室环境中执行各种动作的高精度3D标注数据。
  - **MPI-INF-3DHP [Mehta et al. 2017]**：提供多视角的动作捕捉数据，有助于提升模型在复杂场景中的表现。
- **验证数据集**:
  - **Human3.6M**: 作为主要验证集，用于评估模型的准确性和鲁棒性。

#### 训练配置
- 使用AdamW优化器，初始学习率为$1e^{-4}$，权重衰减为0.01。
- 模型在多个GPU上分布式训练，每批次大小为64。

#### 性能指标
- 在Human3.6M上的MPJPE（Mean Per Joint Position Error）从原始HMR的58.7mm降低到42.3mm，显示了显著的精度提升。此外，在MPI-INF-3DHP上也取得了优异的表现，表明其在不同数据集上的泛化能力。

### 2. Multi-HMR (2024-02-22)

#### 核心原理
- **Patch-level Detection + Offset Regression**: 采用CenterNet范式进行检测阶段处理，允许一次性提出方法而无需复杂的后处理。
- **Human Perception Head (HPH)**: 每个检测到的token作为查询，使用交叉注意力机制预测姿势和形状参数，以及三维空间位置。
- **相机参数嵌入**: 可选地，已知的相机内参通过傅里叶编码表示为射线起源于相机中心的信息。

#### 技术优势
- **高效多人员处理**: 首次实现了单帧中多人全身3D网格重建，无需两步流程（先检测后重建）。
- **高精度**: 通过ViT提取全局特征，提升了对复杂场景的理解能力。
- **灵活性**: 支持可选的相机参数输入，增强了模型在不同场景下的适应性。

#### 缺点分析
- **计算资源需求高**：Multi-HMR使用了ViT作为Backbone，并且支持可选的相机参数输入，这增加了计算复杂度和内存需求。特别是在处理高清图像或多人员场景时，对硬件要求较高。
- **手部细节优化不足**：尽管引入了CUFFS数据集以改善手部关键点的预测精度，但在实际应用中，手部姿态估计的准确性仍有待进一步提高，尤其是在复杂手势或手部遮挡的情况下。
- **模型大小与部署灵活性**：由于采用了复杂的网络结构和多种损失函数，Multi-HMR的整体模型尺寸较大，不利于在资源受限设备上的部署。

#### 数据集
- **训练数据集**:
  - **MuPoTS, 3DPW, EHF**: 这些数据集主要用于评估3D人体网格恢复性能。
  - **BEDLAM, AGORA, CUFFS and UBody**: 用于训练通用模型的数据集组合，其中UBody包含噪声较大的真实世界图像，有助于提升模型的鲁棒性。特别地，CUFFS是一个专门为改善手部细节而设计的合成数据集。
- **验证数据集**:
  - **MuPoTS, CMU, MuCo**: 用于验证模型在多人场景下的性能。

#### 训练配置
- 使用多种重建损失：直接在SMPL-X参数(rot)，生成的顶点(v3d)，结合两种(rot+v3d)，以及添加重投影损失(+v2d)。
- 在不同的分辨率下进行了实验（如896×896），并且使用了不同的骨干网络预训练策略（如DINOv2）。

#### 性能指标
- 在MuPoTS数据集上的PA-MPJPE（Protocol-A Mean Per Joint Position Error）达到了78.8mm，相较于其他方法有明显优势。特别是在处理遮挡问题和多人交互方面，Multi-HMR展现了出色的性能。使用CUFFS数据集进行训练后，对于手部关键点的预测精度有了显著提高。

### 3. GVHMR (2024-09-10)

#### 核心原理
- **Gravity-View Coordinates**: 定义了一个由重力方向和摄像机视角方向组成的坐标系统，以减少重力方向上的累积误差。
- **Rotation Prediction**: 相比于全三维自由度旋转，更直观地估计围绕重力方向的一维旋转。
- **Transformer with Rotary Positional Embedding (RoPE)**: 使用增强的Transformer模型直接回归整个动作序列，更好地捕捉视频帧之间的相对关系，适用于长序列建模。

#### 技术优势
- **世界坐标系重建**: 实现了无需外部传感器即可恢复与重力对齐的全局运动轨迹。
- **抗噪声能力**: 通过重力视图坐标减少了相机运动与人体运动的耦合干扰。
- **实时性能**: RoPE使得模型能够处理无限长序列，并且支持并行推理。

#### 数据集
- **训练数据集**:
  - **AMASS [Mahmood et al. 2019]**：一个大型的人体运动捕捉数据集，主要用于生成合成数据。
  - **BEDLAM [Black et al. 2023]**：包含丰富的全身运动捕捉数据，有助于提升模型对复杂动作的理解能力。
  - **H36M [Ionescu et al. 2014]**：高精度的动作捕捉数据集，常用于人体姿态估计任务。
  - **3DPW [von Marcard et al. 2018]**：提供了真实世界中的多视角人体姿态数据，有助于增强模型的泛化能力。
- **验证数据集**:
  - **RICH [Huang et al. 2022]**：包含静态摄像头捕获的视频序列，总时长为59.1分钟，具有精确的全局人体运动标注。
  - **EMDB-2 [Kaufmann et al. 2023]**：使用移动摄像头拍摄的数据集，包含25个序列，总时长为24.0分钟。

#### 训练配置
- GVHMR拥有12层的Transformer编码器，每个注意力单元有8个头，隐藏维度为512。MLP层使用GELU激活函数。
- 训练过程中采用了多种数据增强技术，如在AMASS数据集中模拟静态和动态相机轨迹，归一化关键点等。
- 序列长度设置为L=120，经过500个epoch后模型收敛，批次大小为256，整个训练过程大约需要13小时在2块RTX 4090 GPU上完成。

#### 性能指标
| 模型 | 数据集 | PA-MPJPE | MPJPE | W-MPJPE | WA-MPJPE |
| --- | --- | --- | --- | --- | --- |
| GVHMR | RICH(24) | 78.8 mm | 126.3 mm | 126.3 mm | 78.8 mm |
| GVHMR | EMDB(24) | 111.0 mm | 276.5 mm | 274.9 mm | 110.6 mm |

具体来说，在RICH和EMDB-2数据集上的表现如下：
- 在RICH数据集上，GVHMR的WA-MPJPE（World-Aligned MPJPE）为78.8mm，W-MPJPE（World-coordinate MPJPE）为126.3mm，显示出较高的准确性。
- 在EMDB-2数据集上，WA-MPJPE为111.0mm，W-MPJPE为276.5mm，同样表现出色，特别是在处理相机移动的情况下，GVHMR能够更准确地捕捉复杂的运动轨迹，并且具有良好的平滑度和稳定性。

#### 缺点分析
- **重力感知依赖性强**：GVHMR的核心优势在于其能够恢复与重力对齐的全局运动轨迹，但这同时也意味着它对环境中的重力方向非常敏感。如果摄像头或传感器未能正确校准重力方向，可能会导致较大的误差。
- **长序列建模挑战**：虽然RoPE机制使得GVHMR能够在理论上处理无限长的视频序列，但在实际应用中，随着序列长度增加，模型训练难度和计算成本也随之上升，可能导致训练不稳定或过拟合问题。
- **数据增强依赖性**：GVHMR在训练过程中广泛使用了数据增强技术（如模拟静态和动态相机轨迹）。然而，过度依赖这些增强手段可能会导致模型在真实世界数据上的泛化能力下降，特别是在面对未见过的数据分布时。

### 4.综合比较

| 方法 | 主要优点 | 主要缺点 |
| --- | --- | --- |
| HMR2.0 | 精度高，鲁棒性好，轻量化设计 | 对复杂遮挡处理能力有限，依赖高质量标注数据，实时性能有待提升 |
| Multi-HMR | 高效多人员处理，高精度，灵活性强 | 计算资源需求高，手部细节优化不足，模型大小与部署灵活性差 |
| GVHMR | 世界坐标系重建，抗噪声能力强，实时性能好 | 重力感知依赖性强，长序列建模挑战大，数据增强依赖性 |
---

## 三、Transformer架构的优势

### 1. 全局信息捕获
- **对比CNN**: CNN主要依赖局部感受野，难以捕捉长距离关节间的关联，而Transformer通过自注意力机制可以同时考虑所有关节间的关系。
- **实证效果**: ViTPose等基于Transformer的方法在COCO数据集上的AP指标较HRNet有显著提升。

### 2. 并行计算效率
- **硬件友好性**: Transformer的矩阵运算非常适合GPU/TPU加速，如ViT-B在NVIDIA A100上可以实现较高的推理速度。
- **扩展性强**: 可以通过调整层数和通道数灵活调整模型大小以适应不同的应用场景需求。

### 3. 灵活的输入输出结构
- **多模态兼容**: Transformer可以轻松处理多种类型的输入（如RGB图像、深度图、IMU数据），并通过SMPLX等参数化模型生成3D网格或关键点输出。

### 4. 自监督预训练潜力
- **MAE/BEiT应用**: 利用掩码图像重建等自监督策略预训练Transformer，减少对标注数据的依赖，从而提升模型泛化能力。

---

## 四、未来发展方向及改进建议

1. **改进遮挡处理能力**：
   - 引入更多先进的遮挡处理算法，例如利用多视角信息进行融合，或者采用生成对抗网络（GANs）来生成缺失部分的人体姿态。

2. **减少对高质量标注数据的依赖**：
   - 探索自监督学习和弱监督学习方法，利用大量未标注数据进行预训练，从而降低对高质量标注数据的依赖。

3. **提升实时性能**：
   - 开发更加高效的Transformer变种，结合知识蒸馏技术，推动姿态估计算法在移动设备上的实时应用。

4. **优化手部细节估计**：
   - 增加专门针对手部动作的数据集，并设计更精细的手部特征提取模块，以提高手部姿态估计的准确性。

5. **增强模型的通用性和适应性**：
   - 设计更具通用性的模型结构，使其能够在不同应用场景中保持较高的性能，同时减小对特定环境条件（如重力方向）的依赖。
---


通过对上述三个SOTA方法的深入分析可以看出，Transformer架构凭借其强大的全局建模能力和灵活性，在人体姿态估计领域展现出巨大潜力。未来的研究将进一步探索如何结合多种先进技术，推动该领域的持续进步。 
